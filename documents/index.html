<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Disaster Tweet Classifier</title>
    <link rel="stylesheet" href="/static/style.css">
</head>
<body>
    <header>
        <h1>Disaster Tweet Classifier</h1>
        <nav>
            <a href="/">Home</a>
            <a href="/pca">PCA Model</a>
            <a href="/bert">BERT Model</a>
            <a href="/svm">SVM Model</a>
        </nav>
    </header>

    <main>
        <section>
            <h2>ğŸ“˜ Project Description</h2>
            <p>In today's digital age, social media platforms like Twitter play a critical role in how people share information during emergencies. This project leverages Natural Language Processing (NLP) and machine learning to automatically classify tweets related to real disasters.</p>
            <p>By training models on thousands of labeled tweets, our system is designed to detect whether a tweet is reporting an actual disaster or simply mentioning related keywords metaphorically. This helps emergency response teams, journalists, and humanitarian organizations identify urgent situations more quickly and accurately.</p>
        </section>

        <section>
            <h2>ğŸ‘¨â€ğŸ’» Team Members</h2>
            <ul>
                <li>Jonathan Tran â€“ Project Manager and Data Analyst</li>
                <li>Kazuki Susuki â€“ Client Liaison and Data Visualizer</li>
                <li>Jeampy Kalambayi â€“ Data Modeler and Documentation Lead</li>
            </ul>
        </section>

        <section>
            <h2>ğŸ¤ Project Client</h2>
            <ul><li>Dr. Cengiz Gunay</li></ul>
        </section>

        <section>
            <h2>ğŸ§° Tools and Technologies</h2>
            <ul>
                <li><strong>Programming Language:</strong> Python</li>
                <li><strong>Libraries:</strong> Pandas, NumPy, Scikit-learn, Seaborn, Matplotlib</li>
                <li><strong>NLP Tools:</strong> TF-IDF Vectorizer, Tokenizer</li>
                <li><strong>ML Models:</strong> Logistic Regression, PCA, BERT, SVM</li>
                <li><strong>Platform:</strong> Jupyter Notebook, Flask, PythonAnywhere</li>
            </ul>
            <p>ğŸ“· Below is a visual overview of the tools used:</p>
            <img src="/static/tools.png" alt="Technologies and Tools">
        </section>

        <section>
            <h2>ğŸ“‚ Dataset Source</h2>
            <p>The dataset used in this project comes from a Kaggle competition titled <strong>"Natural Language Processing with Disaster Tweets."</strong></p>
            <p>It contains over 10,000 tweets that have been manually labeled as either:</p>
            <ul>
                <li>1 (Disaster): Tweet is referring to an actual disaster or emergency event</li>
                <li>0 (Non-Disaster): Tweet uses disaster words metaphorically or in unrelated context</li>
            </ul>
            <p>It includes:</p>
            <ul>
                <li>train.csv â€“ training data</li>
                <li>test.csv â€“ for predictions</li>
                <li>sample_submission.csv â€“ submission format for Kaggle</li>
            </ul>
            <p>ğŸ”— <a href="https://www.kaggle.com/competitions/nlp-getting-started" target="_blank">Click here to view and download the dataset from Kaggle Â»</a></p>


             <p>ğŸ’¾ <a href="/static/dataset.zip" download>Click here to directly download the dataset ZIP file Â»</a></p>

        </section>

        <section>
            <h2>ğŸ“Š Visualizations</h2>
            <p>To better understand the structure and insights of the tweet data, we generated several visualizations. These include:</p>
            <ul>
                <li>Tweet Length Distribution Chart</li>
                <li>Word Cloud of Disaster Tweets</li>
                <li>Model Accuracy Comparison Charts</li>
            </ul>
        </section>

        <section>
            <h2>ğŸ“ Tweet Length Distribution</h2>
            <img src="/static/tweet_length.png" alt="Tweet Length Distribution Chart">
            <p><strong>Explanation:</strong><br>
            The chart shows how tweet lengths vary across the dataset. Peaks indicate common tweet lengths. This helps detect spam or unusually short/long entries.</p>
        </section>

        <section>
            <h2>ğŸŒ©ï¸ Word Cloud: Disaster Tweets</h2>
            <img src="/static/disaster_wordcloud.png" alt="Word Cloud of Disaster Tweets">
            <p><strong>Explanation:</strong><br>
            This word cloud shows the most frequent terms appearing in disaster-related tweets. Larger words are more common.</p>
        </section>

        <section>
            <h2>ğŸ“Œ Accuracy Comparison Number</h2>
            <img src="/static/model_accuracy.png" alt="Accuracy Comparison Chart">
        </section>

        <section>
            <h2>ğŸ“ˆ Model Accuracy Comparison</h2>
            <img src="/static/model_accuracy2.png" alt="Model Accuracy Comparison Chart">
            <p><strong>Explanation:</strong> This chart shows the performance of Logistic Regression, PCA-enhanced models, BERT, and SVM classifiers.</p>
        </section>
    </main>

    <footer>
        <p>Â© 2025 Disaster Tweet Classifier Team</p>
    </footer>
</body>
</html>
